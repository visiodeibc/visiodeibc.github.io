{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"AWS EMR 101","text":"AWS EMR이란? AWS EMR 구성 요소 AWS EMR 수명 주기 궁금증 다음에는 #AWS EMR이란? EMR은 기존에 AWS Elastic Map Reduce의 약자로서 AWS클라우드 상에서 빅 데이터 워크 플로우를 생성하고 관리하기 편리하게 해주는 툴이다. 기존에는 MapReduce가 industry default였지만, 새로운 기술들이 나오면서 이런 환경 및 도구들을 다 편리하게 사용할 수 있도록 해주는 서비스이다. 나도 배워가며 이 글을 쓰고 있기에 개념적 오류가 있을 수도 있으니 혹여나 그런부분이 있으면 편하게 알려주세요~ ❤️. 내가 이해한 바로는 EMR은 **AWS상에서 빅 데이터 ETL, ELT작업들을 쿠버네티스처럼 생성하고, 관리하고, 모니터링하기에 편하도록 해주는 툴이다.** EMR클러스터를 생성할때 기본적으로 툴들을 선택하여 해당 환경이 세팅된 클러스터를 받을 수 있다. 이런 툴로서는 **[Hadeoop, HBase, Presto, Spark]** 등이 있다. 뿐만 아니라 vpc, access key등을 같이 설정하여 보안을 유지하면서 해당 작업을 진행 할 수 있고, 무엇보다 `S3` 상에 있는 데이터들 혹은 `AWS DynamoDB`와 같은 AWS에서 제공하는 데이터 소스들과 함께 작업하기에 용이하다. 설명 링크 #AWS EMR 구성 요소 클러스터: AWS EC2 인스턴스들의 모음 노드: 클러스터 내에 있는 각 인스턴스들 마스터 노드: 데이터와 작업을 배포하고 조정, 클러스터 관리. 모든 클러스터에는 마스터 노드가 있다. 마스터 노드로 단일 클러스터를 생성 할 수 있다. 코어 노드: HDFS에서 작업을 실행하고 데이터를 저장. multi node 클러스터에는 꼭 1개 있음 작업 노드: 작업만 하는 worker 노드. 데이터 저장은 하지 않는다. #AWS EMR 수명 주기 사용자 정의 클러스터 생성 실행 부트스트랩 작업 - 지정한 소프트 웨어 설치 및 환격 구축 사용자 지정 테스트 RUN 클러스터 종료 #궁금증 데이터는 어디서 오지? 보통 S3버킷을 지정하거나 직접 구축되어 있는 Hadoop 클러스터를 가지고 일할 수도 있다. 추가적으로 bootstrap에서 필요항 데이터 로드들도 하는 것으로 보여진다. task 작업은 어디서 오지? aws cli를 통해서 전달하거나 S3에 테스크 파일을 업로드하여 진행할 수 있는거 같다. task 지정 및 관리는 어떻게 하지? airflow, aws step function, 혹은 boto를 사용해서 cron으로도 사용 가능해 보인다. #다음에는실제로 EMR을 활용하여 간단한 테스크를 진행하여 보고록 하겠다. 튜토리얼 링크","link":"/2021/10/24/AWS-EMR-101/"},{"title":"Hello World!","text":"Hello World!","link":"/2021/10/23/Hello_World/"},{"title":"Hexo 101","text":"Hexo 블로그를 만들면서 이런저런 삽질들이 있었다….무엇보다 중요한 기본 명령어들을 잊지 않기 위해서 이 블로그를 기록한다. #Draft 개념기본적으로 글을 쓰면서 바로 deploy할 수 는 없고 그렇다고 다른데서 글을 써서 넘길 수는 없는 처지이다 보니draft를 하기 위해서는 1hexo new draft '&lt;포스트 이름&gt;' 명령어로 글을 쓰면 /source/_drafts 안에 포스팅이 생성된다. 해당 포스팅을 보고 싶을 경우에는 1hexo server --draft 를 할 경우에 draft에 있는 포스팅을 볼 수 있다. #Categories &amp; TagsCategories는 기본적으로 인덱스, 대,중,소 분류의 개념이며, Tags의 경우에는 # 해시 태그의 개념으로 생각하면 될듣 하다.우선 각 글에 categories, tags를 추가하기 위해서는 123456tags:- 태그1- 태그2categories:- 카테고리1- [카테고리2, 카테고리2-1] 위와 같은 형식으로 진행하면 된다. #Publish &amp; Deploydraft를 작성 완료하면 publish 하기 위해서는 1hexo publish '&lt;포스트 이름&gt;' deploy하기 위해서는 1hexo d -g 로 하면 된다.이상 hexo 기본 끝!","link":"/2021/12/07/Hexo-101/"},{"title":"AWS GLUE 101","text":"AWS GLUE란? 페키지 장점 단점 그래서? #AWS GLUE란?AWS Glue는 AWS에 따르면 ‘간단하고 확장 가능한 서버리스 데이터 통합’ 플랫폼이다. 여기서 키워드는 서버리스이다.지난 글 (AWS EMR 101) 에서 다루었던 EMR과 비교하여 보면 좋을거 같다. GLUE와 EMR의 차이는 흡사 RedShift와 Athena의 차이와 비슷하다. Redshift의 경우 데이터 웨어 하우징 서비스로서 스토리지, 인스턴스를 추가 생성해서 데이터를 웨어하우징 하는데 반해 (s3를 직접 보는 옵션도 생겼다.)Athena는 기본적으로 S3 혹은 다른 HDFS시스템에 있는 데이터에 붙어서 고정된 Computing Instance 없이 flexible하게 병렬적으로 쿼리를 실행한다. 이와 같이 EMR의 경우는 지정한 규모의 클러스터를 먼저 생성한 후에[temporarily or running] s3에 있는 스파크 스크립트를 submit하여 돌리고 종료하거나 클러스터는 shrink하는 라이프 사이클을 가진다면, GLUE 같은 경우별도의 클러스터 혹은 인스턴스를 뛰울 필요 없이 서버 없이 불릴때마다 spark가 설치된 환경에서 ETL작업을 진행하는 식으로 사용된다. #페키지GLUE에는 AWS상에서 데이터를 운영하기에 용이한 다양한 피처들이 포함되어 있다. AWS Glue ETL : 가장 기본적인 ETL(Extract, Transform, Load)작업을 담당하는 피처. AWS Glue Data Catalog : 데이터의 전반적인 카탈로그를 보관하는 장소로서, 카탈로그에 등록된 테이블들은 Redshift, Athena, AWS EMR등에서 손쉽게 사용하고 접근할 수 있다. AWS Glue Crawler : 자동으로 저장된 데이터의 스키마를 추출해서 Data Catalog에 저장해준다. AWS Clue DataBrew : 데이터 분석가와 사이언티스트가 코드 작성 없이 다양한 데이터 소스(RDBS, Hadoop, NoSQL 등등)으로 부터 데이터를 정재하고 추출하여 시각화 할 수 있도록 도와 주는 툴. AWS Glue Elastic Views : 여러 데이터 스토어에 있는 데이터를 조합하고 복제하여 서로 가상테이블로 사용할 수 있게 도와준다.(cdc도 지원이 가능한거 같다.)이와 같이 데이터 작읍을 위해 필요한 많은 과정들을 각각의 피처와 제품으로 패키지화 해서 필요한 부분을 선택적으로 사용할 수 있도록 한 서비스이다. #장점 컴퓨팅 리소스, 클러스터를 관리할 필요가 없다. Athena의 장점과 마찬가지로 어느 규모의 클러스터를 사용해야 해당 job을 잘 감당할 수 있을지 굳이 고민하지 않아도 GLUE 자체적으로 서버리스하게 컴퓨팅 리소스를 관리하다 보니 신경을 안써도 된다. S3에 새로운 파일이 적재될때마다 Lambda 함수를 붙여서 GLUE를 트리거 하는게 하나의 추천 유즈 케이스인것을 보았을때, 거의 Real Time 데이터 처리를 할때에 용이해 보인다.[경험해 본적은 없지만 인스턴스에서 spark stream으로도 가능할거 같기는 하지만.] 전반적으로 클릭엔 디플로이 형식으로 GLUE를 운영할 수 있고, Crawlser, ETL서비스들은 심지어 자동으로 코드도 생성해 주다 보니 상대적으로 편리하고 빨리 적용할 수 있다. #단점 서버리스 서비스이다 보니 EMR에 대비해 비슷한 job을 동일한 시간동안 run했을때 가격이 상대적으로 비싸다. 서버리스 이다보다 customization, 즉 클러스터를 뛰워서 직접 조작하고 원하는 부분을 튜닝하는 작업은 할 수 없다. #그래서?전반적으로 데이터 팀 규모가 크지 않고 빠른 아웃풋을 내야하는 상황에서는 매우 매력적으로 보인다. 클러스터 인프라를 직접 관리하지 않아도 되는 면이 매우 좋다.추가적으로 포함된 많은 피처를 입맛데로 가져다 사용해도 되다 보니, 우리 회사도 Athena를 사용하면서 Data Catalog도 사용하고 있다. AWS Glue Elastic Views는 몰랐었는데 한번 테스팅을 해봐야 겠다.나이수","link":"/2021/10/27/AWS-GLUE-101/"},{"title":"Prefect(테스크 오케스트레이션) 101","text":"테스크 오케스트레이션? Prefect? 핵심 개념 Prefect 구조 Task Flow Prefect 서버 Prefect Agent 삽질 포인트 실제 도입시 고민해야할 점 #테스크 오케스트레이션?테스크 오케스트레이션이란 말 그대로 task[작업] + orchestration[지휘]의 합성어로서, 반복되게 실행되어야 하는 작업들을 대리해서 실행해주는 역할을 한다. 데이터 필드에서 ETL, ELT과정에서 주기적으로 돌아야 하는 배치 작업들이 있는데 규모가 작을 때는 리눅스 서버에 cron으로 실행하면 되지만 이 방법의 경우 에러 헨들링, 로깅 등을 직접 관리해줘야 하는 고충이 있다. 추가적으로 데이터 작업이 클러스터 규모로 커지게되면 전반적인 클러스터에게 테스크를 할당하고 관리하는 방법이 필요한데 이를 테스크 오케스트레이션 툴이 담당하고 있다. #Prefect? 가장 많이 알려지고 가장 큰 커뮤니티를 가진 테스트 오케스트레이션 툴로는 `Apache Airflow`가 있다. 디폴트 툴로서의 장점들도 있지만 스케일하며 직면한 문제들을 고치고자 만들어진 툴이 바로 Prefect이다. 프리펙트의 에어플로우보다 나은점을 다룬 아티클은 [여기](https://medium.com/the-prefect-blog/why-not-airflow-4cfa423299c4)에서 확인할 수 있다. 기본적으로 prefect은 오픈소스툴이고 prefect cloud 서버를 사용할시에 과금이 되는 체제다. cloud server를 사용할 경우 업데이트 관리, 직접 운영하는 인프라 리소스가 줄어드는 장점이 있다. 기본적으로 매달 10,000번의 run은 무료 제공이 되고 추후에는 하나의 run마다 과금이 되는데 이 또한 매우 저렴하다. 대부분의 task orchestration툴은 DAG[directed acyclic graph]를 만들기에 용이하다. 데이터 파이프라인에서 task1, task2, task3등으로 구분하여 서로의 디펜던시등을 고려한 플로우를 만들 수 있도록 도와준다. #핵심 개념#Prefect 구조 #Task테스크는 prefect에서 가장 기본적인 work 단위로서 하나의 테스크로 구성됩니다. 기존에 있는 def 함수를 task로 바꾸는 함수는 prefect 에서 제공하는 @task 데코레이터를 붙여주면 됩니다. 예시 12345from prefect import task@taskdef plus_one(x): return x + 1 #Flow플로우는 여러개의 task가 순차적으로 구성되어 있는 컨테이너로 보시면 됩니다. task로 지정된 여러개의 일을 플로우 안에서 실행시켜 줍니다. 예시 1234567891011121314from prefect import task, Task, Flowimport random@taskdef random_number(): return random.randint(0, 100)@taskdef plus_one(x): return x + 1with Flow('My Functional Flow') as flow: r = random_number() y = plus_one(x=r) 위 코드에서는 My Functional Flow라는 이름의 플로우가 생성된고 이 플로우를 실행시키기 위해서키면 random_number가 호출됩니다.plus_one이 호출됩니다. 위와 같이 코드를 만든 후에 flow를 바로 실행시켜 보기 위해서는 flow.run()함수를 호출하면 되고 만일 작성된 함수를 prefect server에 등록하고 관리하고 싶을 경우 flow.register(project_name=&lt;등록된 프로젝트 명&gt;) 를 코드 마지막에 첨부한 후에 해당 python코드를 돌려주면 cloud상에서 해당 테스크가 등록된 것을 확인할 수 있다. #Prefect 서버prefect 서버는 기본적으로 prefect를 관리하는 헤드쿼터라고 보면 됩니다. 기본적인 UI툴이 제공되며, 직접 서버를 호스팅 하고 싶은 경우 원하는 인스턴스, 혹은 ECS로 도커에 돌릴수 있습니다. 또는 prefect에서 각각의 에이전트들이 등록한 플로우를 볼 수 있고, 실패 성공 비율, 프로젝트 구분 및 관리, 테스크 스케쥴링, 등을 관리할 수 있습니다. prefect cloud를 사용할 경우는 상관 없지만 직접 호스팅을 할경우에는 아래와 같은 방법으로 하면 됩니다. **prefect 는 python 3.6이후부터 지원한다. 기기에 python, pip, docker, docker-composer가 설치되어 있는지 확인한다, 안되어 있으면 설치한다. 12345678 #일반pip install prefect#condaconda install -c conda-forge prefect#pipenvpipenv pinstall --pre prefect a. prefect cloud를 사용할 경우 아래와 같은 설정을 해줘야한다. [현재 회사에서는 cloud를 쓰며 계정은 jake에게 문의 하기] 1234567#1 서버 설정prefect backend cloud#2 로그인prefect auth login -k &lt;KEY&gt;#로그인 key는 cloud에서 발급받을 수 있다. cloud를 써야 전사 task를 한번에 모니터링이 가능하기에 cloud를 사용하자 b. 직접 로컬에서 디플로이할 경우에는 아래 명령어로 서버 설정을 해준 후에 서버를 론칭한다. 12345#1 서버 설정prefect backend server#2 서버 시작prefect server start localhost:8080에 가면 prefect 서버를 볼 수 있다. 위 단계는 서버만 가동된 상태라 flow를 등록하고 run해도 실제로 flow를 실행할 agent가 없기에 flow실행이 실패한다. 밑에서 agent도 다구어 줄 것이다. 여기서 agent를 등록하기 위해서는 아래 명령어를 실행해준다. 1prefect agent local start #Prefect Agentagent는 실제로 flow를 실행하는 주체로서 테스크를 돌리는 서버에서 돌고 있다. agent 타입은 총 4가지로 구분된다. local - 그냥 로컬에서 돌고 있는 agent docker - docker에 필요 라이브러리 등등을 말아서 돌고 있는 agent kubernetes - 쿠베네티 상에서 알아서 스케일 아웃 인 할 수 있도록 되는거?[저도 잘 몰라요.. ㅜㅜ] AWS ecs task상에서 도는 agent로 EC2혹은 Fargate상에서 돈다. 우선 이번 파일럿은 feasibility에 초점을 맞추었기에 screen을 활용한 local agent를 사용함. 이때 주의할점은테스크와 플로우로 등록한 작업들은 local agent를 실행한 환경에서 돌기에 virtualenv를 활용할 경우 이 점을 고려해야한다[삽..질… ㅜㅜ].자 그럼 prefect agent를 등록하려면 어떻게 해야할까? 우선 prefect agent를 등록하려면 바라봐야할 prefect server가 있어야 한다. prefect agent를 등록하는 방법은 아래와 같다. 12345678910111213#직접 prefect server를 구현한 경우#위처럼 그냥 바로 로컬 구현한 경우prefect agent local start#다른 인스턴스 혹은 머신데서 돌 경우prefect agent &lt;AGENT TYPE&gt; start --api &lt;API ADDRESS&gt;#클라우드에서 돌 경우1. prefect auth login -k &lt;KEY&gt;2. prefect agent &lt;AGENT TYPE&gt; start#혹은 아래처럼 실행한다prefect agent local start --key &lt;KEY&gt; 에는 local, docker, kubernetes 등이 들어간다. 는 prefect ui에서 Team &gt;&gt; Service Accounts에서 발급 받을 수 있다. 기본적으로 prefect cloud server, prefect backend server 에 따라 각각 디폴트 서버 주소는 https://api.prefect.io, http://localhost:4200 설정 된다. #삽질 포인트 agent 실행시 environment - 당연한 이야기처럼들릴 수도 있지만 등록된 flow는 agent 위에서 프로세스로 돌게된다. 그래서 agent start하는 환경이 flow가 돌아갈 환경이 된다. 가상환경을 사용하는 경우 해당 환경을 실행시키고 agent를 실행시켜야 한다. localagent path설정 - flow는 등록한 환경에서 run되는게 아니다 그렇기에 특정 path에 있는 import가 존재할 경우 flow.run_config에서 working_dir를 특정해 주어야한다. 하나의 테스크 리턴값이 여러개인 경우 - 하나에 테스크에서 여러개의 리턴값을 가지는건 안된다. 생각해보니 데이터 파이프 라인에서 한 테스크가 여러개의 리턴값을 가질 일은 거의 없다. 서빙 서버를 테스크로 뛰우는 것 - 원래는 bentoml서빙 자체를 prefact로 하려고 했지만 서빙을 하면 서버가 뜨는 것이기에 해당 테스크가 끝나지 않는다. 그래서 flow가 계속 돌게 된다. 서버 디플로이는 다른 CI방식을 알아보자. #실제 도입시 고민해야할 점실제로 도입을 고려할 시에는 dependencies 등 파이썬 가상환경 및 설정이 중요할거 같다. 각 테스크 마다 다른 환경들 설정을 따로 agent를 뛰어야 하는지 등의 고민이 추가적으로 필요하다. 지속성과 확장성을 위해서 docker agent를 통한 시도가 불가피해 보이는데 추후에 정리하여 포스팅을 할 것![잊지 말자 ㅎ]","link":"/2021/09/03/Prefect-%ED%85%8C%EC%8A%A4%ED%81%AC-%EC%98%A4%EC%BC%80%EC%8A%A4%ED%8A%B8%EB%A0%88%EC%9D%B4%EC%85%98/"},{"title":"regex 101","text":"#RegEx?RegEx는 regular expression의 약자로 text, file을 서치할때 효율적이고 빠르게 찾을 수 있도록 도움이 되는 친구다.나의 하늘같은 사수님께서 sql 쿼리와 regex는 배워두면 두고두고 써먹을때가 있다고 하셔서 해야하는데… 생각만하다쉽게 가르쳐 주고 알려주는 사이트를 발견하여 이참에 배워두려고 한다.해당 사이트 링크 #Let’s 기릿!#Basic Matcher . 는 모든 캐릭터에 매칭이 된다. 케릭터 사이에 []를 사용해주면 해당 케릭터와 []안에 있는 모든 케릭터에 매칭이 된다. ex)b[ae]r -&gt; bar, ber 케릭터 사이에 [^]를 사용할때 ^뒤에 케릭터를 추가하면 해당 케릭터를 제외한 모든 텍스트가 매칭된다. ex) b[^a]r -&gt; 매치: ber, bur 비매치: bar [x-y] x~y사이에 있는 케릭터가 매치 된다. (숫자, 알파벳 공통) #Repetitions 케릭터 뒤에 * 가 붙으면 바로 앞 케릭터가 없거나 n개 있는 경우까지 매칭된다. ex)be*r -&gt; br, ber, beer 케릭터 뒤에 + 가 붙으면 바로 앞 케릭터가 1 ~ n개 있는 경우까지 매칭된다. ex)be+r -&gt; ber, beer 케릭터 뒤에 ? 가 붙으면 바로 앞 케릭터가 있거나 없는경우 매칭된다. ex)be?r -&gt; br, ber {x}가 있는 경우 x만큼의 숫자만큼 케릭터가 있는 경우가 매칭된다. ex)be{2}r -&gt; beeer {x,}가 있는 경우 x 숫자 이상의 케릭터가 있는 경우가 매칭된다. ex)be{2,}r -&gt; beer, beeer {x,y}가 있는 경우 x, y중간만큼 케릭터가 있는 경우가 매칭된다. ex)be{1,2}r -&gt; ber, beer #Grouping () 를 사용할 경우 grouping이 되는데 이 사이에 있는 케릭터가 그루핑 된다. (ha)-\\1,(haa)-\\2 를 활용하여 referencing이 된다는데 이해를 못했다… |는 or의 표현이다 앞에 있거나 뒤에 있는 표현을 매칭해준다. ex) (C|c)at -&gt; Cat, cat { } [ ] / \\ + * . $^ | ?들은 regex에서 사용하는 예약어 이다. 이 문자를 고르려면 그 앞에 \\를 추가하여야 한다. ex) (\\*|\\.) -&gt; *, . ^의 경우 해당 라인이 ^뒤에 오는 캐릭터로 시작하는 line을 찾아준다. $의 경우 해당 라인이 $앞에 나오는 케릭터로 끝나는 부분을 찾아준다. \\w는 레터, 숫자, 언더스코어를 찾아준다. ex) abcABC123 _ \\W는 레터, 숫자, 언더스코어를 제외하고 찾아준다. ex) .:!? \\d는 숫자를 찾아준다. \\D는 숫자를 제외하고 찾아준다. \\s는 스페이스를 찾아준다. \\S는 스페이스를 제외하고 찾아준다. #Lookaround (?=)는 =뒤에 오는 케릭터와 매치되는 케릭터를 찾아준다. ex) \\d+(?=PM) -&gt; 3PM (?!)는 !뒤에 오는 케릭터와 없는 경우 케릭터를 찾아준다. ex) \\d+(?!PM) -&gt; 매치: 3 비매치: 3PM (?&lt;=) 는 =뒤에 오는 케릭터 뒤에 있는 경우를 매칭해준다. (?&lt;!) 는 !뒤에 오는 케릭터가 없는 경우 매칭해준다. #Flags global flag는 모든 매치를 찾아준다. multiline flag는 각 라인별로 따로 찾아준다. case insensitive 는 대소문자 상관 없이 서치해준다. .*? 를 통해 lazy matching 첫 finding이 매칭 된다. #마치며평소에 막연했던 regex를 이번 튜토리얼을 보며 쉽게 배웠다. 이 튜토리얼 적극 추천!!!","link":"/2021/12/05/Regex-101/"},{"title":"Twitter Clone[사이드 프로젝트 가즈아아아아아]","text":"#뭐라도 해야지…Data Engineer라는 포지션을 유지하면서 모종의 장단점이 있겠지만, 아쉬운 부분은 제품 과의 거리이다.애초에 무엇이든 만들어보고자 개발자의 길을 왔는데, 아무래도 만들어보는 도전이 필요한거 같다.보통 초반에 계시판을 많이 만들어 보는데, 이와 매우 비슷한 트위터를 만들어 볼 예정이다. 가장 기본적인 포스팅 부터 시작하여,좋아요, 댓글, archive 등등 추가 기능들을 구현하며 심도를 늘여갈 예정이다. #목표#기술- 전반적인 Web App 풀스택 경험 - front - vue - back - fastapi, nestjs - dockerization, Devops - docker - docker-compose - aws hosting - gitAction CI/CD - DB 생성, 스키마 설계 및 관리 - mySql - MongoDB 좀 크고 많아 보일 수 있지만 위의 요소들을 이번 프로젝트를 하면서 다루어 보고 배워볼 것이다.이렇게 원대하고 크게 시작할 수 있지만… 실패하고 낙담하여 팀원을 구할지도 모른다.. 흠냐..홧팅. #Wish my luck…fingers crossed… #참고 자료Developing a Single Page App with FastAPI and Vue.js","link":"/2021/11/11/Twitter-clone/"},{"title":"Hexo 이미지 첨부하기","text":"hexo 에서는 이미지 첨부하는 방법은 아래와 같다. #1. source/img 폴더 생성 #2. Markdown에 원하는 이미지 임포트image를 사용하는 방법은 몇가지가 있다. 1234567방법1 - md ![tree](&lt;image_path&gt;)방법2 - html &lt;img src=&lt;image_path&gt; alt=&quot;tree&quot; style=&quot;zoom:50%;&quot;/&gt;image_path = 상대 경로(ex-&quot;/img/Hexo/tree.png&quot;) #3. image center align개인적으로 블로그에서 이미지가 center align되는게 좋은데 항상 center html 설정하기에는 귄찮았다. 이를 수정하기 위해서는 .styl을 수정하면 된다. themes/icarus/include/style/article.styl로 이동한다. &amp;.article 밑에 img 부분에 아래 코드를 추가해준다. 이상 끝!!! 이미지 첨부가 이리 어렵다니…","link":"/2021/10/24/hexo-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%B2%A8%EB%B6%80%ED%95%98%EA%B8%B0/"},{"title":"Pyenv 삽질기… 파이썬 버전관리와 가상환경 구축하는 방법","text":"#여러분은 부디…시간을 아끼시길… pyenv 설치 다운로드 Shell 환경 설정 추가 파이썬 디펜덴시 설치 원하는 파이썬 버전 다운로드 및 글로벌 설정 드디어 파이썬 설치!!!! pyenv-virtualenv설치 git으로 설치하기 brew로 설치하기 virtualenv 생성 및 설정 출처 Photo by Hitesh Choudhary on Unsplash 파이썬으로 작성된 스크립트를 회사에서 사용하기 시작하며 겪게되는 골치 아픈 문제가 있다. 바로 파이썬 버전 관리… 뿐만 아니라 테스크와 프로젝트마다 요구되는 라이브러리와 필요 버전이 다르다 보니 나의 컴퓨터에도 우후죽순 설치된 수많은 버전의 파이썬들이 볼때마다 불편하고 찝찝했다[꼭 방 구석에 쓰레기를 보고 못 본척하는 기분…]. 그리하여 이번에 마음을 먹고 깔끔하게 정리하기로 했다. 고민 중, 회사 동료 분이 추천해주신 pyenv를 사용해보기로 했다. pyenv는 다양한 파이썬 버전을 자유롭게 사용할 수 있도록 할 뿐만 아니라, 매번 만들고 activate 해야하는 프로젝트별 가상환경 문제도 편리하게 해결해준다. 하지만 다양한 기능만큼 초반에 삽질을 반복했기에 이 기록을 남긴다. 우선 시작하기에 앞서서 모든 내용은 pyenv github를 참고하고 작성한 것입니다. 원문을 보시고 싶으신 분들은 여기를 클릭해 주세요. #pyenv 설치#다운로드 맥북 유저의 경우 brew를 사용하여 편하게 설치하실 수 있으신데요. 12brew update brew install pyenv 위와 같은 코드를 terminal에 돌려주면 설치가 됩니다. 설치는 생각보다 오래 걸려요.[한 10분?] 그러니 너무 오래걸려도 잘못된게 아니니 좀만 기다려 주세요! [brew가 없으신 분들을 위해 brew설치법] 👈클릭 2. brew가 없다면 깃허브에서 바로 클론하여 설치도 가능합니다. 1git clone [https://github.com/pyenv/pyenv.git](https://github.com/pyenv/pyenv.git) ~/.pyenv 만약에 특정 폴더에 설치를 원하는 경우 ~/.pyenv 위치에 원하시는 path를 치완해주시면 됩니다. 추가적으로 아래와 같이 다이나믹 bash extension을 사용하여 속도를 높일 수 있는데 선택사항이니 참고해 주세요! 1cd ~/.pyenv &amp;&amp; src/configure &amp;&amp; make -C src 위 코드를 돌릴 경우 위와 같이 path를 달리 했다면 ~/.pyenv를 바꾸는거 잊지 마세요! #Shell 환경 설정자 설치를 완료 했으면 이제 shell에서 pyenv명령어를 알아들을 수 있도록 path에 추가해 주어야 합니다. 사용하시는 shell 종류에 따라 실행해야할 명령어가 달라지니 잘 확인하고 진행해 주세요! bash[debian, ubuntu, mint] 1234567891011echo -e 'if shopt -q login\\_shell; then' \\\\ '\\\\n export PYENV\\_ROOT=&quot;$HOME/.pyenv&quot;' \\\\ '\\\\n export PATH=&quot;$PYENV\\_ROOT/bin:$PATH&quot;' \\\\ '\\\\n eval &quot;$(pyenv init --path)&quot;' \\\\ '\\\\nfi' &gt;&gt; ~/.bashrc echo -e 'if \\[ -z &quot;$BASH\\_VERSION&quot; \\]; then'\\\\ '\\\\n export PYENV\\_ROOT=&quot;$HOME/.pyenv&quot;'\\\\ '\\\\n export PATH=&quot;$PYENV\\_ROOT/bin:$PATH&quot;'\\\\ '\\\\n eval &quot;$(pyenv init --path)&quot;'\\\\ '\\\\nfi' &gt;&gt;~/.profile echo 'eval &quot;$(pyenv init -)&quot;' &gt;&gt; ~/.bashrc zsh brew로 설치한 경우 12echo 'eval &quot;$(pyenv init --path)&quot;' &gt;&gt; ~/.zprofile echo 'eval &quot;$(pyenv init -)&quot;' &gt;&gt; ~/.zshrc git으로 클론한 경우 1234echo ‘export PYENV\\_ROOT=”$HOME/.pyenv”’ &gt;&gt; ~/.zprofile echo ‘export PATH=”$PYENV\\_ROOT/bin:$PATH”’ &gt;&gt; ~/.zprofile echo ‘eval “$(pyenv init — path)”’ &gt;&gt; ~/.zprofile echo 'eval &quot;$(pyenv init -)&quot;' &gt;&gt; ~/.zshrc 자 위 설정이 끝났으면 터미널을 다시 시작하여 줍니다. #추가 파이썬 디펜덴시 설치자 위 설치가 끝난 후 본격적으로 파이썬을 설치하기 전에 파이썬 디펜덴시를 설치해 주세요. Mac OS 1brew install openssl readline sqlite3 xz zlib Ubuntu/Degbian/Mint 123sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \\\\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\\ libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev CentOS 1yum install gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel libffi-devel xz-devel #원하는 파이썬 버전 다운로드 및 글로벌 설정#드디어 파이썬 설치!!!!자 위 설치와 설정을 완료 했다면 이제 원하시는 파이썬 버전을 설치해 주시면 됩니다! 1pyenv install \\[파이썬 버전\\] [파이썬 버전]에 3.7.11과 같은 원하시는 파이썬 버전을 기입하시면 설치가 실행됩니다. 추가적으로 1pyenv versions 위에 명령으로 설치된 모든 파이썬 버전 확인이 가능하며 1pyenv global \\[파이썬 버전\\] [파이썬 버전]에 3.7.11과같은 버전을 입력하여 위 명령어를 실행하면 글로벌 파이썬 버전을 설정하여 줄 수 있습니다! #pyenv-virtualenv설치자 단순히 pyenv를 설치했다고 해서 원하는 파이썬 가상환경을 말들 수 있는게 아니랍니다. 물론 이미 있으신 virtualenv를 사용하여 가상 환경을 구축하여도 되지만 pyenv-virtualenv를 활용하시면 해당 프로젝트 혹은 폴더로 갈때마다 알아서 virtualenv가 로드되어 편하게 이용할 수 있답니다. #git으로 설치하기1$ git clone [https://github.com/pyenv/pyenv-virtualenv.git](https://github.com/pyenv/pyenv-virtualenv.git) $(pyenv root)/plugins/pyenv-virtualenv 위 명령어로 클론해 주세요. 추가적으로 자동 실행을 위해서는 아래와 같이 bashrc에 명령어를 추가해 주세요. 1$ echo ‘eval “$(pyenv virtualenv-init -)”’ &gt;&gt; ~/.bashrc #brew로 설치하기1$ brew install pyenv-virtualenv 위와 같이 설치한 후에 동일하게 path에 추가해 주세요. 12eval “$(pyenv init -)” eval “$(pyenv virtualenv-init -)” 자 이제 모든 설치와 설정이 완료 되었습니다! 이제 가상 환경을 만들고 사용하는 일만 남았네요! #virtualenv 생성 및 설정pyenv 글로벌로 설정되어 있는 파이썬으로 가상환경을 만드려면 단순히 1pyenv virtualenv \\[가상환경 이름\\] 위에 명령어를 실행하여 생성하여 줍니다. 위와 같이 가상환경이 생성되었다면 가상환경을 만들고자 하시는 프로젝트 및 폴더로 이동하여 아래와 같은 명령어를 입력해 주세요. 1pyenv local \\[가상환경 이름\\] 위와 같은 명령어를 실행하면 해당 폴더에 .python-version이라는 파일이 생성되어 해당 가상환경 정보를 담게 됩니다. 이후에는 해당 디렉토리에 들어가면 알아서 해당 가상환경이 실행되게 됩니다. 만약에 글로벌 버전이 아닌 다른 버전에 파이썬으로 가상환경을 만들고 싶은 경우에는 1pyenv virtualenv \\[파이썬 버전\\] \\[가상환경 이름\\] 으로 실행해 주시면 원하는 버전으로 가상환경이 생성되게 됩니다. 자~ 모두 여기까지 하시면 자유롭게 파이썬 버전 설정과 가상환경을 만들어서 사용하실 수 있답니다! 모두 고생하셨습니다~! #출처pyenv git — 링크 pyenv-virtualenv git — 링크 pyenv local — 링크 pyenv command — 링크","link":"/2021/07/21/pyenv_%EC%82%BD%EC%A7%88%EA%B8%B0/"},{"title":"우리 회사는 dbt[data build tool] 를 써야 할까?","text":"#dbt(data build tool)은 무엇이고 언제 써야하는 것일까? dbt란? 왜 dbt를 쓰지? 우리 회사[나]는 왜 dbt를 도입하려고 했나? 그래서? 왜 아직인가? 나란 사람👨🏻‍💻 visiodeibc TL;DR ===== dbt는 sql만으로 데이터 파이프라인을 구성할 수 있게 해주는 툴 dbt를 사용하는 이유 우리회사에 dbt를 도입하는게 좋을까? 우선 아직! #dbt란? “dbt (data build tool) enables data analysts and engineers to transform data in their warehouses.” — dbt blog 위 설명과 같이 dbt는 데이터 분석가와 엔지니어들이 데이터 웨어하우스에서 데이터의 transform을 도와주는 툴입니다. dbt의 매력은 sql 쿼리를 통해서 원하는 테이블을 생성하고 관리할 수 있죠. dbt는 데이터 파이프 라인을 구축하는 ELT(Extract, Load, Trasform), 혹은 ETL(Extract, Transform, Load)에서 T(Transform)에 특화된 툴 인데요. dbt에 대한 좀 더 깊은 분석은 아래 Humphrey님의 글에 매우 잘 정리되어 있어서 참고하면 좋을거 같아요! dbt로 ELT 파이프라인 효율적으로 관리하기www.humphreyahn.dev #배경#왜 dbt를 쓰지?일반적으로 데이터 분석가들은 SQL 쿼리에 능숙하지만, SQL은 인사이트를 뽑을 수 있는 예쁜 테이블이 있을때 비로서 사용할 수 있죠. 깨끗하고 정돈된 결과 테이블을 가지기 위해서는 보통 2가지 방법이 있어요[제가 알기로는?]. 데이터 분석가가 엄청난 길이의 [몇십 혹은 몇백 줄의…] 쿼리를 주기적으로 돌려서 데이터를 추출. 데이터 엔지니어들이 필요한 테이블을 만들어 주는 스크립트를 작성하여 해당 스크립트에서 주기적으로 데이터를 갱신. 전자는 우선 지속 가능한 방법이 아니며 몇백 줄의 쿼리를 짜더라도 지속적으로 업데이트된 정보를 얻기 위해서는 엔지니어가 해당 쿼리를 스크립트화 해서 crontab 혹은airflow로 같은 방법으로 통해서 관리를 해주어야 하죠. 무엇보다 join이 많아지기 시작한 쿼리는 나중에 새로운 인원이 왔을때 온보딩하는데 엄청난 에너지와 시간이 쓰인다는 단점이… 쿨럭 후자는 분석가와 엔지니어가 같이 일해야 하기에 커뮤니케이션이 늘어나 일의 진행 속도는 느려지며 분석가 분이 매번 부탁을 하기가 불편한 경우도 있답니다. dbt는 바로 이 부분에서 분석가가 쿼리를 사용하여 자체적으로 필요한 데이터 마트 테이블을 생성할 수 있게 한답니다! 또한 설정들을 통해 만들어지는 테이블들의 퀄리티 체크, 테스트가 가능하며, 주기적으로 업데이트 되게 지정할 수도 있다. 이를 통해서 분석가는 매번 엔지니어에게 부탁할 필요없이 익숙한 SQL로 분석에 필요한 테이블을 자급자족할 수 있게 되고, 데이터 엔지니어는 다른 업무에 더 집중할 수 있어 조직의 시간과 돈을 아껴줄 수 있는 매우 매력적인 툴이랍니다.[인건비가 가장 비싸다 보니…]. #우리 회사[나]는 왜 dbt를 도입하려고 했나?저희 회사가 최근에 빠른 성장과 함께 본격적인 데이터팀이 갖추어지기 시작하면서 다양한 데이터 정의, 파이프라이닝, 및 분석 업무들도 늘어나기 했는데요. 현재 저희 회사에서는 주기적으로 필요한 데이터들을, AWS Athena에 있는 데이터 웨어 하우스에서 데이터를 추출하고 정리하여 MySQL에 있는 데이터 마트에 적재하여 사용하고 있습니다. 이 과정을 위해서 각 데이터 마트 테이블 별로 python으로 ETL(Extract, Transfer, Load) 스크립트를 작성하여 crontab으로 스케쥴링을 하여 주기적으로 테이블들을 업데이트 해줍니다. 이러한 ETL 작업은 작업마다 차이가 있겠지만 대략 SQL 80%, python 20%로 구성 되어 있어요. ETL*: 필요한 데이터 소스 및 웨어하우스에서 쿼리로 데이터를 가져온[Extract] 후에 판다스나 스파크를 사용해서 요리조리 보기 이쁘고 원하는 정보로 바꾼[Transform] 후에 깔끔하게 데이터 마트에 **적재[Load]**하는 작업 이 20% 파이썬 코드는 로깅, 에러 헨들링 등의 공통 모듈 과 시스템화를 위한 코드이기에, 만일 sql 쿼리만을 가지고 시스템화가 가능하고, 로깅 및 에러 헨들링을 걱정이 없다면 데이터 엔지니어의 스크립트 작업도 줄어들며 분석가도 부탁할 필요 없이 직접 할 수 있게 되는 것이죠. dbt가 하는 일이 바로 이 부분인데 그럼 당연히 솔깃할 수 밖에 없겠지요? “the best code is no code at all” — 가장 좋은 코드는 없는 코드다. 라는 말처럼, 게으름이 미덕인 프로그래머로서 줄일 수 있는 부분을 줄이고, 작업을 간소화할 수 있게 해주는 dbt. 그래서 알아보고 고민해 보았습니다! #그래서? 결론은 아직!!! 사실 이 글을 시작했을 때는 우선 우리는 못 쓴다[기술적으로]!!! 라는 잠정적 결정을 내렸다가, 미련을 못 버리고 리서치와 실습을 조금 더하다보니 오? 우리도 쓸 수 있나? 라는 스탠스로 바뀌어 아직 알아 보고 있습니다. 경과가 있으면 추후에 또 업데이트를 할 수 있도록 할게요! 우선 dbt가 제공하는 편리 및 기능 자체는 사용 가능하다면 도입하는게 너무 당연하다는 판단이 있었지만, 도입을 결정하기 이전에 먼저 몇가지 질문이 있었죠. dbt가 우리 기존 데이터 인프라와 연동하여 도입이 가능한가? dbt에서 우리가 필요한 기능들을 다 지원 하는가? dbt를 도입 했을때 우리 조직에게는 득이 많은가 실이 많은가? #왜 아직인가?우선 dbt는 공식적으로 Postgres, aws Redshift, google BigQuery, Snowflake, Apache Spark, Databricks 데이터 웨어 하우스를 지원합니다. dbt 클라우드 및 CLI를 통하여 위 웨어 하우스들은 쉽게 연동하고 테스트 해볼 수 있죠. 추가적으로 data transfor을 위한 툴인 dbt는 소스 데이터 웨어 하우스에서 데이터를 변형하여 동일한 데이터 웨어 하우스에 적재하도록 만들어 졌지요. 저희 회사 데이터 인프라 같은 경우 로그는 모두 aws firehose를 이용하여 s3 데이터 레이크에 적재가 된 후 매일 etl 배치 작업을 통하여 데이터 웨어하우스용 parquet으로 변환 후 s3에 적재 됩니다. 이 데이터 웨어 하우스는 aws Athena를 활용하여 쿼리 하는데요. 이와 함께 실시간성 및 변하는 계정 정보, 구매 및 환불 정보 등과 같은 정보들은 MySQL db를 통해서 조회되고 사용하는데, 이를 같이 쿼리 하기 위해서 athena 상에서 JDBC 커넥터를 사용하여 MySQL db를 연동해서 같이 사용하고 있죠. 하지만 실질적으로 MySQL에서 직접 쿼리하는 경우도 많아서, 2 data sources(athena, MySQL), 2 data destinations (MySQL, athena) 형태의 데이터 플로우를 가지고 있어요. 부피가 큰 데이터 마트 테이블의 경우 athena에, 간단한 경우 MySQL에 적재하고 있죠. 우선 dbt에서 mySql과 Athena를 공식적으로 지원하고 있지는 않습니다. 하지만 둘다 오픈소스로 만들어진 connector들을 통해 비공식 사용이 가능하죠. 이와 별개로 dbt가 Transform을 위한 툴인 점을 고려할때 2개의 data destinations으로 다르게 옮기는 것은 불가능하죠. 이 점들을 고려할때 현재 우리 인프라와 아직 최적을 적합성을 가지고 있지는 않으며 또한 안정성과 지원 면에서도 아직은 조금 이르지 않을까?라는 생각을 했는데요. 기술적인 고민과 별개로 dbt를 도입하였을때 우리 회사에 득이 많을까라는 질문을 고민했을때도, 현재 회사에서 sql을 사용하는 인원의 비율이 상대적으로 높지 않으며 sql을 사용하는 대부분의 인원이 python을 사용할 줄 아는 점을 고려 했을때 아직 전사적으로 데이터 파이프라이닝에 있어서 dbt를 도입하기는 이르다 라는 결정을 내리게 되었답니다. #나란 사람👨🏻‍💻에누마에서 데이터 엔지니어로 일하고 있다. 스타트업, 사회적 기업, 기독교 신앙에 관심이 많고, 사이드 프로젝트로 ‘사회적 부엉이’ 에서 에코부엉으로 활동하고 있다. 환경 섹터에 사회적 기업들을 발굴하고 조사한다. 선하며 재미로운 일들을 즐기며 찾는다. [ #visiodeibc#나란 사람medium.com ](https://medium.com/visiodeibc)","link":"/2021/10/10/%EC%9A%B0%EB%A6%AC_%ED%9A%8C%EC%82%AC%EB%8A%94_dbt%EB%A5%BC_%EC%8D%A8%EC%95%BC_%ED%95%A0%EA%B9%8C?/"},{"title":"Hexo TOC(Table of Content) 만들기","text":"난 주로 긴 글을 많이 쓰는데 긴 글을 쓰는 경우 각 글들이 어떻게 구성되어 있는지 보고 싶은 경우가 많다.그래서 hexo의 TOC(Table of Content)라이브러리가 있어서 도입 했는데 생각보다 쉬워서 좋다. https://github.com/bubkoo/hexo-toc #방법 blog 디렉토리로 이동 후에 npm 페키기 설치1npm install hexo-toc --save 해당 디렉토리 _config.yml 파일에서 아래와 같은 설정 추가123456789toc: maxdepth: 3 class: toc slugify: transliteration decodeEntities: false anchor: position: after symbol: '#' style: header-anchor 해당 아티클 마크다운에 기호 추가1&lt;!-- toc --&gt; 위 과정을 마치고 빌드하면 적용된 것을 확인해 볼 수 있다.","link":"/2021/10/24/Hexo-TOC-%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"Hexo-요약글","text":"#요약글?어쩌다 Hexo, icarus theme을 사용했는데 모든 글이 full length로 보이다 보니 카테고리에 들어가서 전반적인 글의 흐름을 보고 원하는 글을 찾기가 매우 불편하다.이것저것 찾아보던 중 요약글을 위해서 따로 seperator 를 추가해 줘야하는 껏을 발견했다.매우 귀찮기는 하지만 이미 이 블로그를 세팅하기 위해 너무 많은 노력을 기울였기에 ….방법은 원하는 곳에 1&lt;!-- more --&gt; 를 추가해 주면 된다. 이상!","link":"/2021/10/30/Hexo-%EC%9A%94%EC%95%BD%EA%B8%80/"},{"title":"mac settup 101","text":"#멕북 처음에 세팅하는 방법매번 세팅을 새로 하는게 귀찮아서 최소 무엇을 설치 해야 하는지 기록을 남긴다.우선 ‘본격 macOS에 개발 환경 구축하기’ 에서 기본적인 세팅을 따라한다.‘개발자를 위한 Mac 초기 설정 하기’ 요고도 추가 참고 #설치 어플리케이션왠만한 개발 관련 어플리케이션, 라이브러리는 brew를 통해서 설치가 가능하다. brew 짱!!! brew iterm - brew install iterm zsh oh-my-zsh git - brew install git git-lfs git config –global user.name “Your Name” git config –global user.email “you@your-domain.com“ git config –global core.precomposeunicode true git config –global core.quotepath false docker Sequel Pro - brew install homebrew/cask-versions/sequel-pro-nightly github Desktop - brew install github vscode - brew install visual-studio-code Spotify - brew install spotify xmind - brew install xmind drawio - brew install drawio #설정 fine tuning 드레그는 세손가락으로 iterm 테마 적용, 폰트 설정, 색 설정, 한글 깨짐 방지, 상태바 추가 zsh w/ oh-my-zsh, theme 설정 #라이브러리 및 사용법#fzf단축키 기능⌃ + T 하위 디렉토리 파일 검색⌃ + R 히스토리 검색esc + C 하위 디렉토리 검색 후 이동 #fasd단축키 기능z 디렉토리 이동s 파일 or 디렉토리 검색 #batcat 에 하이라이트 기능 1brew install bat ~/.zshrc에 cat 대신 사용하도록 설정할 수 있습니다. 1alias cat=&quot;bat&quot; #OpenInTerminal1brew cask install openinterminal-lite #주의할 점 필요 폰트들을 잘 설치 하도록 하자. hexo 리포를 받고, cli설치, icarus theme download해야 제대로 돈다.","link":"/2021/12/07/Mac-settup-101/"}],"tags":[{"name":"Data_Engineering","slug":"Data-Engineering","link":"/tags/Data-Engineering/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"EMR","slug":"EMR","link":"/tags/EMR/"},{"name":"Task_Orchestration","slug":"Task-Orchestration","link":"/tags/Task-Orchestration/"},{"name":"Distributed_Computing","slug":"Distributed-Computing","link":"/tags/Distributed-Computing/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"GLUE","slug":"GLUE","link":"/tags/GLUE/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Pyenv","slug":"Pyenv","link":"/tags/Pyenv/"},{"name":"Prefect","slug":"Prefect","link":"/tags/Prefect/"},{"name":"regex","slug":"regex","link":"/tags/regex/"},{"name":"사이드 플젝","slug":"사이드-플젝","link":"/tags/%EC%82%AC%EC%9D%B4%EB%93%9C-%ED%94%8C%EC%A0%9D/"},{"name":"DBT","slug":"DBT","link":"/tags/DBT/"},{"name":"mac, setup","slug":"mac-setup","link":"/tags/mac-setup/"}],"categories":[{"name":"데이터","slug":"데이터","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"블로그","slug":"블로그","link":"/categories/%EB%B8%94%EB%A1%9C%EA%B7%B8/"},{"name":"Environment","slug":"데이터/Environment","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0/Environment/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"EMR","slug":"AWS/EMR","link":"/categories/AWS/EMR/"},{"name":"툴&#x2F;프레임워크","slug":"툴-프레임워크","link":"/categories/%ED%88%B4-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC/"},{"name":"Task Orchestration","slug":"데이터/Task-Orchestration","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0/Task-Orchestration/"},{"name":"etc","slug":"etc","link":"/categories/etc/"},{"name":"GLUE","slug":"AWS/GLUE","link":"/categories/AWS/GLUE/"},{"name":"Twitter Clone","slug":"Twitter-Clone","link":"/categories/Twitter-Clone/"},{"name":"파이썬","slug":"파이썬","link":"/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/"},{"name":"Python","slug":"데이터/Python","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0/Python/"}]}